How to implement the paper:
Paper page - Solving a Million-Step LLM Task with Zero Errors

Overview of the MAKER approach for million-step reliability
MAKER demonstrates that extremely decomposed microagents with stepwise error correction can complete tasks exceeding one million LLM steps with zero errors. It breaks tasks into small subtasks solvable by focused agents, then applies efficient multi-agent voting at every step to catch and correct mistakes. This massively decomposed agentic process enables scale without relying on ever-lower model error rates huggingface.co.

Core ideas to implement
    â€¢ Extreme decomposition: Break the task into atomic subtasks where a correct answer is likely and incorrect answers are less likely. Favor deterministic micro-operations with clear pre/post-conditions huggingface.co.
    â€¢ Microagents: Specialized, narrow agents each responsible for a single subskill or micro-step. Keep prompts and contexts minimal and standardized huggingface.co.
    â€¢ Stepwise error correction: At every micro-step, run a small ensemble (Nâ‰¥3) of independent executions and use voting or consensus. If disagreement occurs, escalate to another pass or a referee agent huggingface.co.
    â€¢ Modularity and isolation: Each subtask has a well-defined interface (inputs, outputs, invariants). This modularity makes error detection and correction efficient huggingface.co.
    â€¢ Scalable orchestration: Use a scheduler that can fan-out micro-steps, aggregate votes, and maintain global state safely across millions of transitions huggingface.co.
    Direct takeaway: high modularity + local voting at every micro-step is the key to zero-error scale huggingface.co.

Architecture blueprint
Components
    â€¢ Task decomposer: Translates the global objective into a DAG of micro-steps with explicit contracts (preconditions, postconditions, invariants) huggingface.co.
    â€¢ Microagent registry: Catalog of narrow agents with capability descriptors and test suites huggingface.co.
    â€¢ Execution manager: Runs ensembles per micro-step, collects outputs, applies voting/consensus, and triggers re-runs or escalation on disagreement huggingface.co.
    â€¢ State store: Versioned global state with transactional updates; supports checkpoints and rollbacks on error huggingface.co.
    â€¢ Verifier/referee agents: Independent validators for outputs that pass consensus but violate invariants or metrics huggingface.co.
    â€¢ Telemetry and audit: Logs per-step inputs, outputs, votes, disagreements, and corrections; supports million-step traceability huggingface.co.
Data contracts
    â€¢ Step schema: {step_id, inputs, expected_output_schema, invariants, timeout, retry_policy}.
    â€¢ Microagent IO: Minimal JSON interfaces with deterministic fields and validation rules.
    â€¢ Consensus record: {candidates, votes, confidence, decision, escalation_path}.

Implementation plan
1. Define the micro-IR and contracts
    â€¢ IR model: Represent the workflow as a DAG of micro-steps with typed inputs/outputs and invariants.
    â€¢ Contracts: Write machine-checkable postconditions for each micro-step. Favor schemas and unit tests.
2. Build the microagents
    â€¢ Narrow prompts: Focus each agent on one subskill; include examples and strict schemas.
    â€¢ Stateless by default: Read inputs from state store; write outputs atomically.
    â€¢ Local tests: For each agent, create synthetic test cases that exercise edge conditions.
3. Ensemble and voting strategy
    â€¢ K-of-N majority: Start with N=3â€“5 samples per step; accept majority agreement under exact-match or schema-normalized equality.
    â€¢ Tie-breaker: Use a deterministic rule (e.g., deterministic re-run, higher-confidence heuristic, or referee agent).
    â€¢ Escalation policy: If consensus fails twice, either decompose further, switch agent, or increase N temporarily.
4. State management and checkpoints
    â€¢ Versioned state: Every step creates a new state version; use optimistic concurrency with invariant checks.
    â€¢ Checkpoints: Persist snapshots at safe milestones; enable rollback on detected violations.
    â€¢ Isolation: Prevent cross-step side effects; microagents cannot mutate state outside their contract.
5. Orchestration and scaling
    â€¢ Scheduler: Execute ready steps in parallel (respecting DAG dependencies). Throttle ensemble fan-out based on resource budgets.
    â€¢ Backpressure: Prioritize steps with high disagreement rates for deeper decomposition.
    â€¢ Observability: Emit structured logs and metrics for disagreement rate, retries, time per step, and invariant violations.
6. Verification and correction
    â€¢ Validators: Run schema validation and invariant checks before accepting consensus outputs.
    â€¢ Referee agent: When validators fail but consensus passed, ask a separate agent to diagnose and propose corrections.
    â€¢ Auto-corrections: For known error patterns, apply deterministic transformations before re-vote.

Minimal scaffold (pseudo-code)
# Step and microagent interfaces
class Step:
    def __init__(self, step_id, inputs, schema, invariants, retries=2):
        ...
class MicroAgent:
    def execute(self, inputs) -> dict:
        # Call LLM with narrow prompt; return JSON
        ...
# Voting and execution
def run_ensemble(agent: MicroAgent, inputs, n=3):
    candidates = [agent.execute(inputs) for _ in range(n)]
    normalized = [normalize(c) for c in candidates]
    decision, votes = majority_vote(normalized)
    if not decision:
        return None, candidates
    return decision, votes
def run_step(step: Step, agent: MicroAgent, state):
    for attempt in range(step.retries + 1):
        decision, votes = run_ensemble(agent, step.inputs)
        if decision and validate_schema(decision, step.schema) and check_invariants(decision, step.invariants, state):
            return apply_state_update(state, decision), {"votes": votes}
        # escalate or decompose further
        agent = escalate_agent_selection(step)
    raise RuntimeError("ConsensusFailed")
# Orchestration loop over DAG
def execute_workflow(dag, registry, state):
    for step in topo_sort(dag):
        agent = registry.pick(step)
        state, meta = run_step(step, agent, state)
        log_step(step, meta, state)
    return state

Metrics and guardrails
    â€¢ Disagreement rate: percentage of steps with non-unanimous ensemble outputs; drive decomposition depth accordingly huggingface.co.
    â€¢ Retry count per step: watch tail latency and identify fragile microagents.
    â€¢ Invariant violation rate: triggers contract refinement or agent retraining

From <https://copilot.microsoft.com/chats/9AaYYdDhfAYF9KKbCJ7oz> 

Scope and objective
You want a concrete, step-by-step implementation of the MAKER-style microagent workflow, explicitly stating how Microsoft Agent Framework (MAF) and Parlant guidelines contribute at each phase. Below is a practical, modular blueprint you can wire into your stack.

Implementation steps with MAF and Parlant contributions
1. Define micro-IR and validators
    â€¢ What you implement:
        â—‹ IR templates: atomic step types (retrieve_passage, extract_field, compare_values, check_constraint, format_output).
        â—‹ Contracts: strict JSON schemas and invariant predicates per step.
    â€¢ MAF contribution:
        â—‹ Tool descriptors: register step-capable tools and LLM skills as MAF actions with typed inputs/outputs.
        â—‹ Capability routing: annotate actions with cost/latency and model strengths to enable planner/executor selection.
    â€¢ Parlant contribution:
        â—‹ Policy schemas: define guideline packs for input eligibility, tool usage constraints, data-handling policies, and sensitive-action flags.
        â—‹ Contract augmentation: attach guideline-required invariants (e.g., â€œevidence_requiredâ€, â€œno_PIIâ€) to relevant IR templates.

2. Construct microagent registry
    â€¢ What you implement:
        â—‹ Narrow agents: each microagent binds to one IR template, with a pinned prompt/model and test suite.
        â—‹ Versioning: prompts, schemas, and validators are versioned and immutable per release.
    â€¢ MAF contribution:
        â—‹ Agent registration: expose each microagent as a MAF service with a stable endpoint and schema validation middleware.
        â—‹ Telemetry hooks: enable per-call logs, latency/cost capture, and error tags for observability.
    â€¢ Parlant contribution:
        â—‹ Guideline annotations: label each microagent with allowed domains, data types, and escalation pathways.
        â—‹ Runtime filters: auto-redaction or denylist integration for agents touching sensitive content.

3. Build the consensus engine (K-of-N)
    â€¢ What you implement:
        â—‹ Normalization: canonical JSON serialization, unit conversion, sorted arrays/keys.
        â—‹ Voting: majority on normalized outputs; tie-breakers; escalation on low confidence.
    â€¢ MAF contribution:
        â—‹ Fan-out orchestration: run N parallel agent calls; aggregate results; apply backpressure based on system load.
        â—‹ Alternate routing: dynamically swap to alternate agents/models on disagreement or validator failure.
    â€¢ Parlant contribution:
        â—‹ Consensus policies: require unanimous votes for safety-critical steps; downgrade acceptance thresholds when guidelines permit.
        â—‹ Escalation rules: invoke a referee flow if a guideline violation appears (e.g., PII detected, missing evidence).

4. Design the task decomposer
    â€¢ What you implement:
        â—‹ Heuristics: split tasks until each step has a single decision, bounded outputs, and deterministic validators.
        â—‹ DAG builder: dependencies and step ordering with explicit data flows.
    â€¢ MAF contribution:
        â—‹ Planner agent: a stronger model in MAF generates the initial DAG IR using tool capability descriptors.
        â—‹ Cost-aware planning: leverage action metadata to enforce latency/cost budgets in step expansion.
    â€¢ Parlant contribution:
        â—‹ IR guideline pass: validate the DAG against tool eligibility, dependency coherence, schema completeness; rewrite unsafe steps to safer patterns.
        â—‹ Policy-derived decomposition: enforce finer-grained steps when a guideline requires stronger validation (e.g., separate â€œverify_citationsâ€ step).

5. Integrate guideline passes (input, IR, runtime)
    â€¢ What you implement:
        â—‹ Three passes: input sanitization, IR validation/rewrite, runtime output checks and redaction.
    â€¢ MAF contribution:
        â—‹ Pre-/post-call middleware: enforce guideline checks before executing a tool and after producing an output.
        â—‹ Confirmation gates: route interactive confirmations through MAF when guidelines require user consent for sensitive actions.
    â€¢ Parlant contribution:
        â—‹ Rule execution: apply packs that deny, rewrite, or annotate requests; attach audit trails (rule hits, rewrites, denials).
        â—‹ Runtime guards: predicate functions bound to steps (no_PII, evidence_required, allowed_sources_only).

6. Orchestrate execution with checkpoints
    â€¢ What you implement:
        â—‹ Topo execution: run ready nodes; checkpoint state after each accepted result; rollback on invariant or guideline failure.
        â—‹ Adaptive N: tune ensemble size based on disagreement metrics.
    â€¢ MAF contribution:
        â—‹ Scheduler: parallelize independent steps; throttle based on resource budget; transactional state updates.
        â—‹ Checkpoint store: persist per-step inputs/outputs, votes, and guideline artifacts in MAFâ€™s storage or your DB.
    â€¢ Parlant contribution:
        â—‹ Checkpoint validation: assert all guideline invariants before committing; log violations; trigger escalation flows.
        â—‹ Auditability: persist guideline decisions with reason codes for later review.

7. Observability and metrics
    â€¢ What you implement:
        â—‹ Signals: disagreement rate, retry counts, invariant violations, guideline denial rate, end-to-end latency/cost.
        â—‹ Auto-tuning: adjust ensemble N, choose alternate agents, or further decompose steps when signals cross thresholds.
    â€¢ MAF contribution:
        â—‹ Tracing/export: structured logs per call; correlation IDs; metrics export to your observability stack.
        â—‹ Policy hooks: feed metrics back to the planner/executor for routing decisions.
    â€¢ Parlant contribution:
        â—‹ Compliance dashboards: aggregate guideline hits/violations by rule; identify hot spots for policy refinement.
        â—‹ Drift detection: flag rising rates of redaction or denial that signal prompt/tool drift.

Concrete scaffolds
IR step template
# Contracts
IRStep = {
  "step_id": "extract_key_points",
  "op": "extract",
  "inputs": {"text": "...", "schema_hint": {"bullet_max": 6}},
  "expected_schema": {"type": "object", "properties": {"points": {"type": "array", "items": {"type": "string"}}}, "required": ["points"]},
  "invariants": ["evidence_required", "no_pii"],
  "allowed_tools": ["llm.extractor.v1"],
  "deps": ["retrieve_passage"]
}
MAF agent registration (conceptual)
# Register microagent with typed IO and guideline middleware
maf.register_action(
  name="llm.extractor.v1",
  input_schema={"text": "string", "schema_hint": {"bullet_max": "int"}},
  output_schema={"points": ["string"]},
  before_call=[parlant.input_guard],     # input-level guideline pass
  after_call=[parlant.runtime_guard],    # runtime guideline checks (no_pii, evidence_required)
  model="gpt-4o-mini",                   # executor model
  cost_hint={"tokens_per_call": 2_000, "latency_ms": 800}
)
Consensus engine (majority vote)
def normalize(output):
  return {"points": sorted(set([p.strip() for p in output["points"]]))}
def majority_vote(outputs):
  from collections import Counter
  keys = [json.dumps(normalize(o), sort_keys=True) for o in outputs]
  c = Counter(keys)
  winner, count = c.most_common(1)[0]
  return count >= (len(outputs)//2 + 1), json.loads(winner), dict(c)
Execution step with MAF + Parlant
def execute_step(step, n=3, retries=1):
  for attempt in range(retries + 1):
    samples = [maf.call(step["allowed_tools"][0], step["inputs"]) for _ in range(n)]
    decided, chosen, votes = majority_vote(samples)
    if not decided:
      maf.route_alternate(step)  # swap to alternate agent/model
      continue
    # Parlant runtime invariants
    ok, violations = parlant.runtime_guard(chosen, step["invariants"])
    if not ok:
      # escalate to referee flow
      chosen = maf.call("referee.correct.v1", {"samples": samples, "violations": violations, "contract": step})
      continue
    # Schema validation (programmatic)
    validate_schema(chosen, step["expected_schema"])
    checkpoint(step["step_id"], chosen, votes, violations)
    return chosen
  raise RuntimeError(f"ConsensusFailed: {step['step_id']}")

Example end-to-end flow (evidence-grounded summary)
    â€¢ Decomposer (MAF planner):
        â—‹ Plan: retrieve_passage â†’ extract_key_points â†’ verify_evidence_links â†’ merge_summary â†’ format_report.
        â—‹ Cost-aware routing selects small models for execution; stronger model for planning/referee.
    â€¢ Guideline passes (Parlant):
        â—‹ Input: deny speculative domains; sanitize personal data.
        â—‹ IR: require verify_evidence_links step; enforce allowed_sources_only.
        â—‹ Runtime: no_pii, evidence_required for each claim; redact sensitive strings.
    â€¢ Consensus:
        â—‹ N=3 for extract_key_points; unanimous required for verify_evidence_links.
        â—‹ Escalate to referee on any citation mismatch or guideline violation.
    â€¢ Checkpointing:
        â—‹ Persist per-step artifacts; rollback if invariants fail.

Operational tuning
    â€¢ Adaptive ensemble (MAF):
        â—‹ Increase N when disagreement rate rises; reduce for deterministic transforms.
        â—‹ Route to alternate agents on repeated failures; switch models for critical steps.
    â€¢ Policy refinement (Parlant):
        â—‹ Tighten rules for high-risk categories; add new invariants based on observed violations.
        â—‹ Track guideline denial/redaction rates to spot drift.

Direct answer
    â€¢ MAF handles orchestration, execution, routing, parallelization, telemetry, and checkpoints at each step.
    â€¢ Parlant enforces policy and correctness at input, IR, and runtime: it validates, rewrites, escalates, and records guideline decisions that gate acceptance.
    â€¢ Together, they implement MAKERâ€™s guarantees: extreme decomposition, per-step consensus, and strict guardrails that prevent error propagation across millions of steps.
If you share your current MAF action catalog and your guideline packs, I can turn this into a runnable repo scaffold with concrete modules for the decomposer, consensus engine, and guideline middleware.

From <https://copilot.microsoft.com/chats/9AaYYdDhfAYF9KKbCJ7oz> 


Great synthesis question, Chris. Letâ€™s be precise about where the LLM Compiler pattern and the Plan-and-Execute pattern fit in the MAKER-style implementation with MAF + Parlant.

ğŸ§© Do you need both?
    â€¢ Yes, but for different layers of the workflow. 
        â—‹ LLM Compiler pattern gives you structure, contracts, and reproducibility. Itâ€™s the backbone: parse â†’ IR â†’ optimize â†’ execute, with Parlant guidelines enforcing correctness at each stage.
        â—‹ Plan-and-Execute pattern adds adaptability and iterative refinement. Itâ€™s the â€œagent loopâ€ that kicks in when execution results deviate, invariants fail, or guidelines force a rewrite.

âš™ï¸ Where each pattern fits in the pipeline
1. Frontend Parsing & IR Construction
    â€¢ Use LLM Compiler. 
        â—‹ Parse user input into IR steps with schemas and invariants.
        â—‹ Parlant guidelines validate and rewrite unsafe IR.
    â€¢ Why: You need deterministic, auditable artifacts before execution.

2. Optimization Passes
    â€¢ Use LLM Compiler. 
        â—‹ Apply retrieval augmentation, schema enforcement, cost/latency tuning.
        â—‹ Guidelines act as compiler passes (policy enforcement, tool eligibility).
    â€¢ Why: This stage is about systematic transformation, not adaptive re-planning.

3. Execution of IR Steps
    â€¢ Primary: LLM Compiler. 
        â—‹ Run microagents with ensemble consensus, validators, and guideline runtime guards.
    â€¢ Fallback: Plan-and-Execute. 
        â—‹ If consensus fails or invariants are violated, trigger a re-plan loop: 
            Â§ Generate a new plan for the failing step/subgraph.
            Â§ Execute revised steps.
    â€¢ Why: Compiler ensures correctness; plan-and-execute provides resilience.

4. Adaptive Re-Planning
    â€¢ Use Plan-and-Execute. 
        â—‹ When guidelines deny an action, or repeated invariant violations occur, switch to adaptive planning.
        â—‹ Stronger planner agent (via MAF) generates a new sequence of micro-steps.
    â€¢ Why: This is where flexibility mattersâ€”compiler alone would just fail.

5. Observability & Audit
    â€¢ Use LLM Compiler. 
        â—‹ Log IR versions, guideline passes, consensus outcomes.
    â€¢ Use Plan-and-Execute selectively. 
        â—‹ Record re-planning events and adaptive changes for traceability.
    â€¢ Why: Compiler provides reproducibility; plan-and-execute explains deviations.

âœ… Summary
    â€¢ LLM Compiler pattern: Core pipeline (parse, IR, optimize, execute).
    â€¢ Plan-and-Execute pattern: Adaptive loop when execution fails or guidelines force re-planning.
    â€¢ Together: Compiler ensures structure and policy compliance; Plan-and-Execute ensures adaptability and robustness.

Chris, since youâ€™re designing modular RAG pipelines, would you like me to sketch a layered diagram showing exactly how Compiler and Plan-and-Execute interleave with MAF orchestration and Parlant guidelines? That would give you a reusable integration story for your documentation.

From <https://copilot.microsoft.com/chats/9AaYYdDhfAYF9KKbCJ7oz> 


Thatâ€™s a sharp observation, Chris â€” and youâ€™re right: Plan-and-Execute can be extremely useful at the very beginning of a workflow when the userâ€™s query is underspecified or ambiguous. Let me break down why and how it complements the LLM Compiler pattern at that stage.

ğŸ” Why Plan-and-Execute at the Beginning?
    â€¢ Eliciting hidden requirements: 
Many user queries are surface-level (â€œsummarize this datasetâ€) but conceal deeper needs (â€œcompare across time, highlight anomalies, ensure complianceâ€). 
        â—‹ Plan-and-Execute shines here because the planner agent can generate a tentative multi-step plan, then step back to ask clarifying questions before execution.
    â€¢ Adaptive clarification: 
Unlike the Compiler pattern, which assumes the input is already well-formed, Plan-and-Execute can pause, re-plan, and refine based on user feedback.
    â€¢ Efficiency: 
By clarifying upfront, you avoid wasted execution cycles and reduce retries downstream.

âš™ï¸ How It Fits With the Compiler Pattern
Think of the two patterns as layers:
    1. Plan-and-Execute (Front-End Discovery)
        â—‹ Generate an initial plan from the raw query.
        â—‹ Ask clarifying questions if requirements are vague.
        â—‹ Iterate until the plan reflects the real intent.
        â—‹ Example: â€œSummarize Kubernetes logsâ€ â†’ Planner asks: â€œDo you want comparison across clusters, or just raw summaries?â€
    2. LLM Compiler (Structured Execution)
        â—‹ Once intent is clarified, convert the plan into a formal IR with schemas and invariants.
        â—‹ Apply guideline passes (Parlant) to validate and optimize.
        â—‹ Execute with microagents, consensus, and checkpoints (MAF orchestration).

ğŸ“Š Stage-by-Stage Use
    â€¢ Stage 0: User Query Intake â†’ Plan-and-Execute 
        â—‹ Clarify intent, elicit hidden requirements, generate draft plan.
    â€¢ Stage 1: IR Construction â†’ LLM Compiler 
        â—‹ Parse clarified plan into IR, enforce guidelines.
    â€¢ Stage 2: Optimization Passes â†’ LLM Compiler 
        â—‹ Add retrieval, schema enforcement, cost/latency tuning.
    â€¢ Stage 3: Execution â†’ LLM Compiler with fallback Plan-and-Execute 
        â—‹ Run microagents; if invariants fail, re-plan adaptively.

âœ… Takeaway
    â€¢ Plan-and-Execute at the beginning = discovery, clarification, adaptive questioning.
    â€¢ LLM Compiler afterward = structure, reproducibility, policy compliance.
    â€¢ Together, they form a two-phase pipeline: 
        â—‹ Phase 1 (Plan-and-Execute): uncover the real needs.
        â—‹ Phase 2 (LLM Compiler): execute with rigor and guardrails.

Chris, would you like me to sketch a hybrid workflow diagram showing how the â€œclarification loopâ€ (Plan-and-Execute) feeds into the â€œcompiler pipelineâ€ (LLM Compiler)? It would make the interplay between discovery and execution crystal clear for your documentation.

From <https://copilot.microsoft.com/chats/9AaYYdDhfAYF9KKbCJ7oz> 

Excellent connection, Chris. Letâ€™s compare the LLM Compiler + Plan-and-Execute patterns with the Double Diamond design model and Six Thinking Hats framework, then explore how they can be integrated into your agentic workflows.

ğŸ”‘ Comparison
1. Double Diamond (Discover â†’ Define â†’ Develop â†’ Deliver)
    â€¢ Discover: Divergent exploration of the problem space.
    â€¢ Define: Convergent clarification of the real problem.
    â€¢ Develop: Divergent ideation of solutions.
    â€¢ Deliver: Convergent execution and refinement.
Mapping to LLM patterns:
    â€¢ Discover â†’ Plan-and-Execute (front-end discovery): elicit hidden requirements, ask clarifying questions.
    â€¢ Define â†’ LLM Compiler (IR construction): formalize intent into structured IR with guidelines.
    â€¢ Develop â†’ Optimization passes (Compiler) + adaptive re-planning (Plan-and-Execute): generate multiple solution paths, enrich with RAG, prune.
    â€¢ Deliver â†’ Execution (Compiler) with consensus and guideline enforcement: converge on validated outputs.

2. Six Thinking Hats (Parallel perspectives)
    â€¢ White Hat (Facts): Retrieval, evidence grounding.
    â€¢ Red Hat (Feelings): User intent, emotional tone, clarifications.
    â€¢ Black Hat (Caution): Parlant guidelines, invariants, risk checks.
    â€¢ Yellow Hat (Benefits): Optimization passes, efficiency gains.
    â€¢ Green Hat (Creativity): Plan-and-Execute re-planning, divergent solution generation.
    â€¢ Blue Hat (Process): Compiler orchestration, MAF scheduling, consensus engine.
Mapping to LLM patterns:
    â€¢ Hats become roles of microagents or guideline passes.
    â€¢ Example: 
        â—‹ White Hat agent = retriever microagent.
        â—‹ Black Hat = guideline validator.
        â—‹ Green Hat = planner agent generating alternative IRs.
        â—‹ Blue Hat = orchestrator (MAF scheduler + Compiler pipeline).

ğŸš€ Integration Opportunity
By combining these:
    â€¢ Double Diamond gives the macro process flow (divergent â†’ convergent cycles).
    â€¢ Six Hats provide micro-perspectives (parallel checks and balances).
    â€¢ LLM Compiler ensures structure and reproducibility.
    â€¢ Plan-and-Execute ensures adaptability and clarification.
    â€¢ Parlant guidelines act as the Black Hat across all stages.
    â€¢ MAF orchestrates the Blue Hat process control.

ğŸ“Š Hybrid Workflow Example
    1. Discover (Diverge)
        â—‹ Plan-and-Execute: planner agent generates draft plan.
        â—‹ Green Hat: creative exploration of possible decompositions.
        â—‹ Red Hat: ask clarifying questions about user intent.
    2. Define (Converge)
        â—‹ LLM Compiler: parse into IR, enforce schemas.
        â—‹ Black Hat: guideline validation, risk checks.
        â—‹ White Hat: fact retrieval to ground requirements.
    3. Develop (Diverge)
        â—‹ Optimization passes: generate multiple enriched IRs.
        â—‹ Green Hat: propose alternative strategies.
        â—‹ Yellow Hat: highlight efficiency gains.
    4. Deliver (Converge)
        â—‹ Execution: run microagents with consensus.
        â—‹ Blue Hat: orchestrator ensures process control.
        â—‹ Black Hat: runtime guideline enforcement.

âœ… Takeaway
    â€¢ Double Diamond = macro-level divergence/convergence cycles.
    â€¢ Six Hats = micro-level perspectives embedded as agents/guideline passes.
    â€¢ LLM Compiler + Plan-and-Execute = technical backbone for structured execution and adaptive clarification.
    â€¢ MAF + Parlant = orchestration and guardrails.
Together, they form a design-informed agentic workflow: clarify intent (Discover/Define), explore solutions (Develop), converge on validated execution (Deliver), with hats ensuring balanced perspectives at each stage.

